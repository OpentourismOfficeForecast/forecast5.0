{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Lenzerheide\n",
    "\n",
    "## Cross-validation in XGBoost example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd \n",
    "class_data = pd.read_csv(\"insert_csv_file.csv\") \n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1], \n",
    " label=churn_data.month_5_still_here) \n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}  # change learner \n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4,\n",
    " num_boost_round=10, metrics=\"error\", as_pandas=True) \n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear base learners example: learning API only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "X, y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=123)\n",
    "DM_train = xgb.DMatrix(data=X_train,label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test,label=y_test)\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=10)\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "boston_data = pd.read_csv(\"boston_data.csv\")\n",
    "X,y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1] \n",
    "boston_dmatrix = xgb.DMatrix(data=X,label=y) \n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":4} \n",
    "l1_params = [1,10,100]\n",
    "rmses_l1=[]\n",
    "for reg in l1_params:\n",
    "   params[\"alpha\"] = reg\n",
    "   cv_results = xgb.cv(dtrain=boston_dmatrix, params=params,nfold=4, \n",
    "                       num_boost_round=10,metrics=\"rmse\",as_pandas=True,seed=123) \n",
    "   rmses_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
    "print(\"Best rmse as a function of l1:\") \n",
    "print(pd.DataFrame(list(zip(l1_params,rmses_l1)), columns=[\"l1\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untuned model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    "import numpy as np \n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\") \n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], \n",
    "       housing_data[housing_data.columns.tolist()[-1]] \n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y) \n",
    "untuned_params={\"objective\":\"reg:linear\"} \n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "       params=untuned_params,nfold=4, \n",
    " metrics=\"rmse\",as_pandas=True,seed=123) \n",
    "print(\"Untuned rmse: %f\" %((untuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:27:26.509721Z",
     "start_time": "2019-11-29T18:27:26.506736Z"
    }
   },
   "source": [
    "## Tuned model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    "import numpy as np \n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\") \n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], \n",
    "    housing_data[housing_data.columns.tolist()[-1]] \n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y) \n",
    "tuned_params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3, \n",
    " 'learning_rate': 0.1, 'max_depth': 5} \n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "    params=tuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\", \n",
    " as_pandas=True, seed=123) \n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addtional possiblity: tune eta → learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 tree tunable parameters → see slides part 3 number 6 <br>\n",
    "3 linear tunable parameters → see slides part 3 number 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    "import numpy as np \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\") \n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]],  \n",
    "      housing_data[housing_data.columns.tolist()[-1] \n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y) \n",
    "gbm_param_grid = {'learning_rate': [0.01,0.1,0.5,0.9], \n",
    "                 'n_estimators': [200], \n",
    " 'subsample': [0.3, 0.5, 0.9]} \n",
    "gbm = xgb.XGBRegressor() \n",
    "grid_mse = GridSearchCV(estimator=gbm,param_grid=gbm_param_grid,  \n",
    "           scoring='neg_mean_squared_error', cv=4, verbose=1) \n",
    "grid_mse.fit(X, y) \n",
    "print(\"Best parameters found: \",grid_mse.best_params_) \n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:31:14.495585Z",
     "start_time": "2019-11-29T18:31:14.491596Z"
    }
   },
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First example\n",
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    "import numpy as np \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\") \n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], \n",
    "     housing_data[housing_data.columns.tolist()[-1]] \n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y) \n",
    "gbm_param_grid = {'learning_rate': np.arange(0.05,1.05,.05), \n",
    "                 'n_estimators': [200], \n",
    " 'subsample': np.arange(0.05,1.05,.05)} \n",
    "gbm = xgb.XGBRegressor() \n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,  \n",
    "                       n_iter=25, scoring='neg_mean_squared_error', cv=4, verbose=1) \n",
    "randomized_mse.fit(X, y) \n",
    "print(\"Best parameters found: \",randomized_mse.best_params_) \n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second example\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=gbm, scor-\n",
    "ing=\"neg_mean_squared_error\", n_iter=5, cv=4, verbose=1)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:36:33.029147Z",
     "start_time": "2019-11-29T18:36:33.026187Z"
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:41:04.316154Z",
     "start_time": "2019-11-29T18:41:04.311132Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-4-fef785cb252a>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-fef785cb252a>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    → scaling function\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import cross_val_score \n",
    "names = [\"crime\",\"zone\",\"industry\",\"charles\",\"no\",\"rooms\", \n",
    "       \"age\", \"distance\",\"radial\",\"tax\",\"pupil\",\"aam\",\"lower\",\"med_price\"] \n",
    "\n",
    "data = pd.read_csv(\"boston_housing.csv\",names=names) \n",
    "X, y = data.iloc[:,:-1], data.iloc[:,-1] \n",
    "rf_pipeline = Pipeline[(\"st_scaler\",  \n",
    "               StandardScaler()), \n",
    "               (\"rf_model\",RandomForestRegressor())] \n",
    "\n",
    "scores = cross_val_score(rf_pipeline,X,y,     \n",
    "scoring=\"neg_mean_squared_error\",cv=10) \n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "print(\"Final RMSE:\", final_avg_rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- LabelEncoder and OneHotEncoder or in one step → DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns I: LabelEncoder \n",
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(value=0)\n",
    "# Create a boolean mask for categorical columns \n",
    "categorical_mask = (df.dtypes == object)  # true → is categorical\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "# Print the head of the categorical columns \n",
    "print(df[categorical_columns].head())\n",
    "# Create LabelEncoder object: le \n",
    "le = LabelEncoder()\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "# Print the head of the LabelEncoded categorical columns print(df[categorical_columns].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded df_encoded = ohe.fit_transform(df)\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe print(df_encoded[:5, :])\n",
    "# Print the shape of the original DataFrame \n",
    "print(df.shape)\n",
    "# Print the shape of the transformed array \n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DictVectorizer (one step)\n",
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# Convert df into a dictionary: df_dict \n",
    "df_dict = df.to_dict(\"records\")\n",
    "# Create the DictVectorizer object: dv \n",
    "dv = DictVectorizer(sparse=False)\n",
    "# Apply dv on df: df_encoded \n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "# Print the resulting first five rows print(df_encoded[:5,:])\n",
    "# Print the vocabulary \n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "names = [\"crime\",\"zone\",\"industry\",\"charles\",\"no\",\"rooms\",\"age\", \n",
    "        \"distance\",\"radial\",\"tax\",\"pupil\",\"aam\",\"lower\",\"med_price\"] \n",
    "data = pd.read_csv(\"boston_housing.csv\",names=names)\n",
    "X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "xgb_pipeline = Pipeline[(\"st_scaler\", StandardScaler()), \n",
    "                        (\"xgb_model\",xgb.XGBRegressor())]\n",
    "scores = cross_val_score(xgb_pipeline, X, y, \n",
    "                         scoring=\"neg_mean_squared_error\",cv=10) \n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "print(\"Final XGB RMSE:\", final_avg_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer from sklearn.pipeline import Pipeline\n",
    "# Fill LotFrontage missing values with 0 X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)), \n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "# Create the pipeline: xgb_pipeline \n",
    "xgb_pipeline = Pipeline(steps)\n",
    "# Fit the pipeline \n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score\n",
    "# Fill LotFrontage missing values with 0 X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)), \n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "# Create the pipeline: xgb_pipeline xgb_pipeline = Pipeline(steps)\n",
    "# Cross-validate the model cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict('records'), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scor es))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper from sklearn_pandas import CategoricalImputer\n",
    "# Check number of nulls in each feature column nulls_per_column = X.isnull().sum() print(nulls_per_column)\n",
    "# Create a boolean mask for categorical columns categorical_feature_mask = X.dtypes == object\n",
    "# Get list of categorical column names categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "# Get list of non-categorical column names non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "# Apply numeric imputer \n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "[([numeric_feature],Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns], \n",
    "                                            input_df=True, \n",
    "                                            df_out=True\n",
    "                                           )\n",
    "# Apply categorical imputer categorical_imputation_mapper = DataFrameMapper( \n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns], \n",
    "                                                input_df=True, \n",
    "                                                df_out=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# Combine the numeric and categorical transformations numeric_categorical_union = FeatureUnion([ \n",
    "                                          (\"num_mapper\", numeric_imputation_mapper), \n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper) \n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union), \n",
    "                     (\"dictifier\", Dictifier()), \n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)), \n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3)) \n",
    "                    ])\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning XGBoost hyperparameters in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  ...: import xgboost as xgb\n",
    "  ...: import numpy as np\n",
    "  ...: from sklearn.preprocessing import StandardScaler\n",
    "  ...: from sklearn.pipeline import Pipeline\n",
    "  ...: from sklearn.model_selection import RandomizedSearchCV\n",
    "names = [\"crime\",\"zone\",\"industry\",\"charles\",\"no\", \n",
    "  ...: \"rooms\",\"age\", \"distance\",\"radial\",\"tax\",\n",
    "  ...: \"pupil\",\"aam\",\"lower\",\"med_price\"]\n",
    "data = pd.read_csv(\"boston_housing.csv\",names=names)\n",
    "X, y = data.iloc[:,:-1],data.iloc[:,-1] \n",
    "xgb_pipeline = Pipeline[(\"st_scaler\",\n",
    "  ...: StandardScaler()), (\"xgb_model\",xgb.XGBRegressor())] \n",
    "gbm_param_grid = {\n",
    "  ...:     'xgb_model__subsample': np.arange(.05, 1, .05),\n",
    "  ...:     'xgb_model__max_depth': np.arange(3,20,1),\n",
    "  ...:     'xgb_model__colsample_bytree': np.arange(.1,1.05,.05) }\n",
    "randomized_neg_mse = RandomizedSearchCV(estimator=xgb_pipeline, \n",
    "  ...: param_distributions=gbm_param_grid, n_iter=10,\n",
    "  ...: scoring='neg_mean_squared_error', cv=4)\n",
    "randomized_neg_mse.fit(X, y)\n",
    "print(\"Best rmse: \", np.sqrt(np.abs(randomized_neg_mse.best_score_))\n",
    "print(\"Best model: \", randomized_neg_mse.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05), \n",
    "    'clf__max_depth': np.arange(3, 10, 1), \n",
    "    'clf__n_estimators': np.arange(50, 200, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible add-ons\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline, param_distributions=gbm_param_grid, n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "# Fit the estimator \n",
    "randomized_roc_auc.fit(X, y)\n",
    "# Compute metrics \n",
    "print(randomized_roc_auc.best_score_) \n",
    "print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
